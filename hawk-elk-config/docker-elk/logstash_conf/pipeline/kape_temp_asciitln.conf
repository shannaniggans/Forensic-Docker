input {
    file {
        path => "/data/*temp_ascii.tln"
        start_position => "beginning"
        sincedb_path => "/dev/null"
        mode => "read"
    }
}
filter {
    if [message] =~ /^\s*$/ {   #Delete any blank lines and dont send to csv filter
        drop {}
        #Theoutput from KAPE has a lot of lines in it that need to be removed before parsing. Have to add a few lines for that.
	}
    if [message] =~ /^\*/ {
        drop {}
    }
    if [message] =~ /^Hive/ {
        drop {}
    }
    mutate {                    #This is to remove characters that logstash doesnt like that might appear in subject lines
	gsub => ["message", "\[", "\ "]
	gsub => ["message", "\]", "\ "]
    gsub => ["message", "\{", "\("]
	gsub => ["message", "\}", "\)"]
	}
    csv {
        id => "tln"
        separator => "|"
        columns => ["Date","Type","host","Source File","information"]
	 }
    date {
        match => ["Date", "UNIX"] #pay particular attention to the syntax here
        timezone => "UTC" #Update to match the correct timezone       
        target => "@timestamp"
    }
}

output {
    elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "tempasciitln-bonelobster"
    }
    stdout { codec => rubydebug }
}
